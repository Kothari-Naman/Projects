{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook requires 2 files, The xray images in a compressed numpy array format and the csv file containing the reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-09T14:52:23.626512Z",
     "iopub.status.busy": "2025-08-09T14:52:23.625798Z",
     "iopub.status.idle": "2025-08-09T14:52:24.019147Z",
     "shell.execute_reply": "2025-08-09T14:52:24.018318Z",
     "shell.execute_reply.started": "2025-08-09T14:52:23.626475Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/best_finetuned/pytorch/default/1/best (2).pt\n",
      "/kaggle/input/best_model_normal_training/pytorch/default/1/best.pt\n",
      "/kaggle/input/padchest-array/PadChestPA_1980_eng.csv\n",
      "/kaggle/input/padchest-array/all_img.npz\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T14:52:26.817329Z",
     "iopub.status.busy": "2025-08-09T14:52:26.816970Z",
     "iopub.status.idle": "2025-08-09T14:53:04.581321Z",
     "shell.execute_reply": "2025-08-09T14:53:04.580585Z",
     "shell.execute_reply.started": "2025-08-09T14:52:26.817308Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 14:52:47.683391: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754751168.063950      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754751168.167756      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset, random_split\n",
    "from transformers import (\n",
    "    ViTModel,\n",
    "    AutoImageProcessor,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Config,\n",
    "    AutoTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T14:53:04.583099Z",
     "iopub.status.busy": "2025-08-09T14:53:04.582424Z",
     "iopub.status.idle": "2025-08-09T14:53:36.216818Z",
     "shell.execute_reply": "2025-08-09T14:53:36.216002Z",
     "shell.execute_reply.started": "2025-08-09T14:53:04.583079Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "imgs = torch.tensor(np.load(\"/kaggle/input/padchest-array/all_img.npz\")[\"arr_0\"]).unsqueeze(1).repeat(1,3,1,1).to(torch.float16)\n",
    "df = pd.read_csv('/kaggle/input/padchest-array/PadChestPA_1980_eng.csv')\n",
    "all_reports = df.Eng_Reports.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-08-09T14:53:36.218017Z",
     "iopub.status.busy": "2025-08-09T14:53:36.217688Z",
     "iopub.status.idle": "2025-08-09T14:53:36.338146Z",
     "shell.execute_reply": "2025-08-09T14:53:36.337397Z",
     "shell.execute_reply.started": "2025-08-09T14:53:36.217993Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# This function cleans the english reports and removes unwanted words and phrases that might cause the model to hallucinate.\n",
    "def clean_medical_report(text):\n",
    "    \n",
    "    # Skip if input is already empty or None\n",
    "    if not text or not text.strip():\n",
    "        return None\n",
    "    \n",
    "    # Remove formatting artifacts\n",
    "    text = re.sub(r'\\*\\*Original Report:\\*\\*', '', text)\n",
    "    text = re.sub(r'\\*\\*.*?\\*\\*', '', text)  # Remove **text**\n",
    "    text = re.sub(r'---.*?---', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'\\*+', '', text)  # Remove asterisks\n",
    "    # Remove metadata patterns\n",
    "    text = re.sub(r'Comparison with.*?dated.*?\\d{1,2}-\\d{1,2}-\\d{4}', '', text)\n",
    "    \n",
    "    # Clean quotes and whitespace\n",
    "    text = re.sub(r'^\\s*\"', '', text)  # Remove starting quote\n",
    "    text = re.sub(r'\"\\s*$', '', text)  # Remove ending quote\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Clean whitespace\n",
    "    \n",
    "    # **KEY ADDITION: Check if text becomes empty after cleaning**\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        return None  # Return None instead of empty string\n",
    "    \n",
    "    # Ensure proper sentence ending\n",
    "    if text and not text.endswith(('.', '!', '?')):\n",
    "        text += '.'\n",
    "    \n",
    "    return text\n",
    "\n",
    "cleaned_reports = []\n",
    "for report in all_reports:\n",
    "    cleaned = clean_medical_report(report)\n",
    "    if cleaned:  # Only append non-empty strings\n",
    "        cleaned_reports.append(cleaned)\n",
    "\n",
    "print(f\"Original: {len(all_reports)} reports\")\n",
    "print(f\"Cleaned: {len(cleaned_reports)} reports\")\n",
    "print(f\"Removed: {len(all_reports) - len(cleaned_reports)} empty/invalid reports\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T14:53:36.340294Z",
     "iopub.status.busy": "2025-08-09T14:53:36.340104Z",
     "iopub.status.idle": "2025-08-09T14:53:41.959540Z",
     "shell.execute_reply": "2025-08-09T14:53:41.958772Z",
     "shell.execute_reply.started": "2025-08-09T14:53:36.340279Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af0e7c0437ea4737a68368c170afcf9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b98aad3e2ac246ccad10261a1566c34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ebb0db01b94fdda7ff03de91b5da4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/22.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f74598781c42bba59a5510f6b4e73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552f455f60004ff39c621ff2ec79fffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['transformer.h.0.crossattention.c_attn.bias', 'transformer.h.0.crossattention.c_attn.weight', 'transformer.h.0.crossattention.c_proj.bias', 'transformer.h.0.crossattention.c_proj.weight', 'transformer.h.0.crossattention.q_attn.bias', 'transformer.h.0.crossattention.q_attn.weight', 'transformer.h.0.ln_cross_attn.bias', 'transformer.h.0.ln_cross_attn.weight', 'transformer.h.1.crossattention.c_attn.bias', 'transformer.h.1.crossattention.c_attn.weight', 'transformer.h.1.crossattention.c_proj.bias', 'transformer.h.1.crossattention.c_proj.weight', 'transformer.h.1.crossattention.q_attn.bias', 'transformer.h.1.crossattention.q_attn.weight', 'transformer.h.1.ln_cross_attn.bias', 'transformer.h.1.ln_cross_attn.weight', 'transformer.h.2.crossattention.c_attn.bias', 'transformer.h.2.crossattention.c_attn.weight', 'transformer.h.2.crossattention.c_proj.bias', 'transformer.h.2.crossattention.c_proj.weight', 'transformer.h.2.crossattention.q_attn.bias', 'transformer.h.2.crossattention.q_attn.weight', 'transformer.h.2.ln_cross_attn.bias', 'transformer.h.2.ln_cross_attn.weight', 'transformer.h.3.crossattention.c_attn.bias', 'transformer.h.3.crossattention.c_attn.weight', 'transformer.h.3.crossattention.c_proj.bias', 'transformer.h.3.crossattention.c_proj.weight', 'transformer.h.3.crossattention.q_attn.bias', 'transformer.h.3.crossattention.q_attn.weight', 'transformer.h.3.ln_cross_attn.bias', 'transformer.h.3.ln_cross_attn.weight', 'transformer.h.4.crossattention.c_attn.bias', 'transformer.h.4.crossattention.c_attn.weight', 'transformer.h.4.crossattention.c_proj.bias', 'transformer.h.4.crossattention.c_proj.weight', 'transformer.h.4.crossattention.q_attn.bias', 'transformer.h.4.crossattention.q_attn.weight', 'transformer.h.4.ln_cross_attn.bias', 'transformer.h.4.ln_cross_attn.weight', 'transformer.h.5.crossattention.c_attn.bias', 'transformer.h.5.crossattention.c_attn.weight', 'transformer.h.5.crossattention.c_proj.bias', 'transformer.h.5.crossattention.c_proj.weight', 'transformer.h.5.crossattention.q_attn.bias', 'transformer.h.5.crossattention.q_attn.weight', 'transformer.h.5.ln_cross_attn.bias', 'transformer.h.5.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e6f03c584af4bd4908aede1b24d0fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4ad9e710654d2981a48d0b2ef9bfa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8a00a0c6924bdb806355229433337f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5f6634ec8f413b97a21f44431c53fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab562c5169c947f48939d36c8f252d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vit_name = \"WinKawaks/vit-tiny-patch16-224\"  \n",
    "image_processor = AutoImageProcessor.from_pretrained(vit_name)\n",
    "vit_encoder = ViTModel.from_pretrained(vit_name)\n",
    "\n",
    "# GPT-2 small with cross-attention enabled to accept encoder outputs\n",
    "gpt2_cfg = GPT2Config.from_pretrained(\"distilgpt2\")\n",
    "gpt2_cfg.add_cross_attention = True            # enable cross-attn safely\n",
    "text_decoder = GPT2LMHeadModel.from_pretrained(\"distilgpt2\", config=gpt2_cfg)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T14:53:41.960629Z",
     "iopub.status.busy": "2025-08-09T14:53:41.960347Z",
     "iopub.status.idle": "2025-08-09T14:53:41.966136Z",
     "shell.execute_reply": "2025-08-09T14:53:41.965081Z",
     "shell.execute_reply.started": "2025-08-09T14:53:41.960604Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Simple projection from ViT encoder hidden dim to GPT-2 hidden dim\n",
    "enc_dim = vit_encoder.config.hidden_size\n",
    "dec_dim = text_decoder.config.n_embd\n",
    "proj = nn.Linear(enc_dim, dec_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T14:53:41.967658Z",
     "iopub.status.busy": "2025-08-09T14:53:41.966991Z",
     "iopub.status.idle": "2025-08-09T14:53:41.982588Z",
     "shell.execute_reply": "2025-08-09T14:53:41.981845Z",
     "shell.execute_reply.started": "2025-08-09T14:53:41.967631Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# main model\n",
    "class VisionTextModel(nn.Module):\n",
    "    def __init__(self,vit,proj,gpt2):\n",
    "        super().__init__() \n",
    "        self.vit = vit\n",
    "        self.proj = proj\n",
    "        self.gpt2 = gpt2\n",
    "\n",
    "    def forward(self, pixel_values, input_ids, attention_mask, labels=None):\n",
    "\n",
    "        vit_out = self.vit(pixel_values=pixel_values)\n",
    "        gpt_in = self.proj(vit_out.last_hidden_state)\n",
    "\n",
    "        outputs = self.gpt2(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            encoder_hidden_states=gpt_in,\n",
    "            encoder_attention_mask=torch.ones(gpt_in.size()[:-1], dtype=torch.long, device=gpt_in.device),\n",
    "            labels=labels\n",
    "        )\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T05:00:40.756408Z",
     "iopub.status.busy": "2025-08-09T05:00:40.756087Z",
     "iopub.status.idle": "2025-08-09T05:00:41.121210Z",
     "shell.execute_reply": "2025-08-09T05:00:41.120468Z",
     "shell.execute_reply.started": "2025-08-09T05:00:40.756382Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): VisionTextModel(\n",
       "    (vit): ViTModel(\n",
       "      (embeddings): ViTEmbeddings(\n",
       "        (patch_embeddings): ViTPatchEmbeddings(\n",
       "          (projection): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (encoder): ViTEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x ViTLayer(\n",
       "            (attention): ViTAttention(\n",
       "              (attention): ViTSelfAttention(\n",
       "                (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "              )\n",
       "              (output): ViTSelfOutput(\n",
       "                (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): ViTIntermediate(\n",
       "              (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): ViTOutput(\n",
       "              (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (layernorm_before): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "            (layernorm_after): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layernorm): LayerNorm((192,), eps=1e-12, elementwise_affine=True)\n",
       "      (pooler): ViTPooler(\n",
       "        (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (proj): Linear(in_features=192, out_features=768, bias=True)\n",
       "    (gpt2): GPT2LMHeadModel(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-5): 6 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): Conv1D(nf=2304, nx=768)\n",
       "              (c_proj): Conv1D(nf=768, nx=768)\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (crossattention): GPT2Attention(\n",
       "              (c_attn): Conv1D(nf=1536, nx=768)\n",
       "              (q_attn): Conv1D(nf=768, nx=768)\n",
       "              (c_proj): Conv1D(nf=768, nx=768)\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D(nf=3072, nx=768)\n",
       "              (c_proj): Conv1D(nf=768, nx=3072)\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = VisionTextModel(vit_encoder, proj, text_decoder).to(device)\n",
    "model.gpt2.gradient_checkpointing_enable()\n",
    "model.vit.gradient_checkpointing_enable()\n",
    "model.gpt2.config.use_cache = False\n",
    "model = nn.DataParallel(model)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T14:53:46.570925Z",
     "iopub.status.busy": "2025-08-09T14:53:46.570724Z",
     "iopub.status.idle": "2025-08-09T14:53:48.501790Z",
     "shell.execute_reply": "2025-08-09T14:53:48.500984Z",
     "shell.execute_reply.started": "2025-08-09T14:53:46.570910Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "enc = tokenizer(\n",
    "    cleaned_reports,\n",
    "    max_length=384,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "input_ids = enc[\"input_ids\"]          \n",
    "attention_mask = enc[\"attention_mask\"] \n",
    "labels = input_ids.clone()\n",
    "labels[attention_mask == 0] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T14:56:23.632023Z",
     "iopub.status.busy": "2025-08-09T14:56:23.631751Z",
     "iopub.status.idle": "2025-08-09T14:56:23.636138Z",
     "shell.execute_reply": "2025-08-09T14:56:23.635492Z",
     "shell.execute_reply.started": "2025-08-09T14:56:23.632002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "lr = 2e-5\n",
    "weight_decay = 0.02\n",
    "batch_size = 32  \n",
    "grad_accum_steps = 2\n",
    "use_fp16 = True\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T14:56:23.838056Z",
     "iopub.status.busy": "2025-08-09T14:56:23.837801Z",
     "iopub.status.idle": "2025-08-09T14:56:23.843298Z",
     "shell.execute_reply": "2025-08-09T14:56:23.842661Z",
     "shell.execute_reply.started": "2025-08-09T14:56:23.838034Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = TensorDataset(imgs, input_ids, attention_mask, labels)\n",
    "train_ds, test_ds = random_split(dataset, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n",
    "train_loader,test_loader = DataLoader(train_ds,batch_size=batch_size,shuffle=True),DataLoader(test_ds,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T03:15:02.809324Z",
     "iopub.status.busy": "2025-08-09T03:15:02.808684Z",
     "iopub.status.idle": "2025-08-09T03:15:02.823330Z",
     "shell.execute_reply": "2025-08-09T03:15:02.822495Z",
     "shell.execute_reply.started": "2025-08-09T03:15:02.809297Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/2926929195.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_fp16)\n"
     ]
    }
   ],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\"] # its a good practice to not apply weight decay or l2 regularization to bias and layernorm layers for stable training.\n",
    "param_groups = [\n",
    "    {\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "     \"weight_decay\": weight_decay},\n",
    "    {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "     \"weight_decay\": 0.0},\n",
    "]\n",
    "optimizer = torch.optim.AdamW(param_groups, lr=lr)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T03:15:03.387639Z",
     "iopub.status.busy": "2025-08-09T03:15:03.386719Z",
     "iopub.status.idle": "2025-08-09T03:15:03.392190Z",
     "shell.execute_reply": "2025-08-09T03:15:03.391316Z",
     "shell.execute_reply.started": "2025-08-09T03:15:03.387611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for p in model.module.vit.parameters():# freeze the encoder\n",
    "    p.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T03:15:20.968566Z",
     "iopub.status.busy": "2025-08-09T03:15:20.967880Z",
     "iopub.status.idle": "2025-08-09T04:40:14.815218Z",
     "shell.execute_reply": "2025-08-09T04:40:14.814636Z",
     "shell.execute_reply.started": "2025-08-09T03:15:20.968532Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1:   0%|          | 0/236 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 done | avg_train_loss=3.3294 | avg_test_loss=2.4118\n",
      "Saved improved checkpoint to /kaggle/working/epoch_001.pt (eval_loss=2.4118)\n",
      "Updated best checkpoint: /kaggle/working/best.pt (eval_loss=2.4118)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 done | avg_train_loss=2.4431 | avg_test_loss=2.1230\n",
      "Saved improved checkpoint to /kaggle/working/epoch_002.pt (eval_loss=2.1230)\n",
      "Updated best checkpoint: /kaggle/working/best.pt (eval_loss=2.1230)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 done | avg_train_loss=2.2110 | avg_test_loss=1.9783\n",
      "Saved improved checkpoint to /kaggle/working/epoch_003.pt (eval_loss=1.9783)\n",
      "Updated best checkpoint: /kaggle/working/best.pt (eval_loss=1.9783)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 done | avg_train_loss=2.0736 | avg_test_loss=1.8994\n",
      "Saved improved checkpoint to /kaggle/working/epoch_004.pt (eval_loss=1.8994)\n",
      "Removed old improved checkpoint: epoch_001.pt\n",
      "Updated best checkpoint: /kaggle/working/best.pt (eval_loss=1.8994)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 done | avg_train_loss=1.9596 | avg_test_loss=1.8345\n",
      "Saved improved checkpoint to /kaggle/working/epoch_005.pt (eval_loss=1.8345)\n",
      "Removed old improved checkpoint: epoch_002.pt\n",
      "Updated best checkpoint: /kaggle/working/best.pt (eval_loss=1.8345)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 done | avg_train_loss=1.8776 | avg_test_loss=1.7899\n",
      "Saved improved checkpoint to /kaggle/working/epoch_006.pt (eval_loss=1.7899)\n",
      "Removed old improved checkpoint: epoch_003.pt\n",
      "Updated best checkpoint: /kaggle/working/best.pt (eval_loss=1.7899)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 done | avg_train_loss=1.8136 | avg_test_loss=1.7560\n",
      "Saved improved checkpoint to /kaggle/working/epoch_007.pt (eval_loss=1.7560)\n",
      "Removed old improved checkpoint: epoch_004.pt\n",
      "Updated best checkpoint: /kaggle/working/best.pt (eval_loss=1.7560)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 done | avg_train_loss=1.7448 | avg_test_loss=1.7333\n",
      "Saved improved checkpoint to /kaggle/working/epoch_008.pt (eval_loss=1.7333)\n",
      "Removed old improved checkpoint: epoch_005.pt\n",
      "Updated best checkpoint: /kaggle/working/best.pt (eval_loss=1.7333)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 done | avg_train_loss=1.6815 | avg_test_loss=1.7098\n",
      "Saved improved checkpoint to /kaggle/working/epoch_009.pt (eval_loss=1.7098)\n",
      "Removed old improved checkpoint: epoch_006.pt\n",
      "Updated best checkpoint: /kaggle/working/best.pt (eval_loss=1.7098)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 done | avg_train_loss=1.6358 | avg_test_loss=1.6879\n",
      "Saved improved checkpoint to /kaggle/working/epoch_010.pt (eval_loss=1.6879)\n",
      "Removed old improved checkpoint: epoch_007.pt\n",
      "Updated best checkpoint: /kaggle/working/best.pt (eval_loss=1.6879)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 done | avg_train_loss=1.5890 | avg_test_loss=1.6741\n",
      "Saved improved checkpoint to /kaggle/working/epoch_011.pt (eval_loss=1.6741)\n",
      "Removed old improved checkpoint: epoch_008.pt\n",
      "Updated best checkpoint: /kaggle/working/best.pt (eval_loss=1.6741)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 done | avg_train_loss=1.5491 | avg_test_loss=1.6744\n",
      "No improvement (eval_loss=1.6744 >= best=1.6741), skipping save\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 done | avg_train_loss=1.5006 | avg_test_loss=1.6661\n",
      "Saved improved checkpoint to /kaggle/working/epoch_013.pt (eval_loss=1.6661)\n",
      "Removed old improved checkpoint: epoch_009.pt\n",
      "Updated best checkpoint: /kaggle/working/best.pt (eval_loss=1.6661)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 done | avg_train_loss=1.4637 | avg_test_loss=1.6574\n",
      "Saved improved checkpoint to /kaggle/working/epoch_014.pt (eval_loss=1.6574)\n",
      "Removed old improved checkpoint: epoch_010.pt\n",
      "Updated best checkpoint: /kaggle/working/best.pt (eval_loss=1.6574)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 done | avg_train_loss=1.4224 | avg_test_loss=1.6603\n",
      "No improvement (eval_loss=1.6603 >= best=1.6574), skipping save\n",
      "Training complete.\n",
      "Saved loss plot to /kaggle/working/loss_curve.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tqdm as tqdm\n",
    "import glob\n",
    "ckpt_dir = \"/kaggle/working/\"\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "best_eval_loss = math.inf\n",
    "best_ckpt_path = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for step, batch in tqdm.tqdm(\n",
    "        enumerate(train_loader, start=1),\n",
    "        total=len(train_loader),\n",
    "        leave=False,\n",
    "        desc=f\"Train {epoch+1}\"\n",
    "    ):\n",
    "        img, input_ids, attention_mask, labels = batch\n",
    "        img = img.to(device, non_blocking=True)\n",
    "        input_ids = input_ids.to(device, non_blocking=True)\n",
    "        attention_mask = attention_mask.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=\"cuda\",enabled=use_fp16): # mixed precision training for faster training\n",
    "            outputs = model(\n",
    "                pixel_values=img,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels,\n",
    "            )\n",
    "            loss = outputs.loss\n",
    "            if loss.dim() > 0:\n",
    "                loss = loss.mean()\n",
    "            loss = loss / grad_accum_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        running_loss += loss.item() * grad_accum_steps\n",
    "\n",
    "        if step % grad_accum_steps == 0:# Graadient accumulation to effectively use large batch size on small GPUs\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        \n",
    "    epoch_train_loss = running_loss / max(1, len(train_loader))\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    model.eval()\n",
    "    test_running_loss = 0.0\n",
    "    test_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(\n",
    "            test_loader,\n",
    "            total=len(test_loader),\n",
    "            leave=False,\n",
    "            desc=f\"Eval {epoch+1}\"\n",
    "        ):\n",
    "            img, input_ids, attention_mask, labels = batch\n",
    "            img = img.to(device, non_blocking=True)\n",
    "            input_ids = input_ids.to(device, non_blocking=True)\n",
    "            attention_mask = attention_mask.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\",enabled=use_fp16):\n",
    "                outputs = model(\n",
    "                    pixel_values=img,\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                if loss.dim() > 0:\n",
    "                    loss = loss.mean()\n",
    "\n",
    "            test_running_loss += loss.item()\n",
    "            test_batches += 1\n",
    "\n",
    "    epoch_test_loss = test_running_loss / max(1, test_batches)\n",
    "    eval_losses.append(epoch_test_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} done | avg_train_loss={epoch_train_loss:.4f} | avg_test_loss={epoch_test_loss:.4f}\")\n",
    "\n",
    "    if epoch_test_loss < best_eval_loss:\n",
    "        # Update best eval loss\n",
    "        best_eval_loss = epoch_test_loss\n",
    "        \n",
    "        # Save the improved model\n",
    "        ckpt_path = os.path.join(ckpt_dir, f\"epoch_{epoch+1:03d}.pt\")\n",
    "        save_obj = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scaler_state_dict\": scaler.state_dict(),\n",
    "            \"avg_train_loss\": epoch_train_loss,\n",
    "            \"avg_eval_loss\": epoch_test_loss,\n",
    "            \"grad_accum_steps\": grad_accum_steps,\n",
    "            \"max_grad_norm\": max_grad_norm,\n",
    "            \"use_fp16\": use_fp16,\n",
    "        }\n",
    "        torch.save(save_obj, ckpt_path)\n",
    "        print(f\"Saved improved checkpoint to {ckpt_path} (eval_loss={epoch_test_loss:.4f})\")\n",
    "        \n",
    "        # Keep only latest 3 improved checkpoints\n",
    "        checkpoint_pattern = os.path.join(ckpt_dir, \"epoch_*.pt\")\n",
    "        checkpoint_files = glob.glob(checkpoint_pattern)\n",
    "        \n",
    "        if len(checkpoint_files) > 3:  \n",
    "            # Sort by modification time (newest first)\n",
    "            checkpoint_files.sort(key=os.path.getmtime, reverse=True)\n",
    "            \n",
    "            # Remove old checkpoints (keep only the latest 3)\n",
    "            old_checkpoints = checkpoint_files[3:]\n",
    "            \n",
    "            for old_ckpt in old_checkpoints:\n",
    "                try:\n",
    "                    os.remove(old_ckpt)\n",
    "                    print(f\"Removed old improved checkpoint: {os.path.basename(old_ckpt)}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error removing {os.path.basename(old_ckpt)}: {e}\")\n",
    "        \n",
    "        # Also save as best.pt (the most recent improvement)\n",
    "        best_ckpt_path = os.path.join(ckpt_dir, \"best.pt\")\n",
    "        torch.save(save_obj, best_ckpt_path)\n",
    "        print(f\"Updated best checkpoint: {best_ckpt_path} (eval_loss={best_eval_loss:.4f})\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"No improvement (eval_loss={epoch_test_loss:.4f} >= best={best_eval_loss:.4f}), skipping save\")\n",
    "    \n",
    "print(\"Training complete.\")\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(range(1, len(train_losses)+1), train_losses, label=\"Train loss\")\n",
    "plt.plot(range(1, len(eval_losses)+1), eval_losses, label=\"Eval loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training/Eval Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plot_path = \"/kaggle/working/loss_curve.png\"\n",
    "plt.savefig(plot_path, bbox_inches=\"tight\", dpi=150)\n",
    "print(f\"Saved loss plot to {plot_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T06:44:59.719072Z",
     "iopub.status.busy": "2025-08-09T06:44:59.718145Z",
     "iopub.status.idle": "2025-08-09T06:45:01.424511Z",
     "shell.execute_reply": "2025-08-09T06:45:01.423645Z",
     "shell.execute_reply.started": "2025-08-09T06:44:59.719046Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SINGLE IMAGE RESULTS ---\n",
      "Predicted: changed in radiologic interpretation. The original report is unchanged from the prior examination.\" \"The heart size appears normal with no cardiomegaly or mediastinal shift observed, and there are clear lungs bilaterally consistent for acute cardiac failure (CVC). There's a small right pleural effusion on both sides\n",
      "________________________\n",
      "Ground Truth: Opacification of the left lung base is indeterminate, with a possible retrocardiac mass suggestive of a neumonadic process. Further diagnostic workup, including radiologist consultation after completion of treatment, is recommended.\n"
     ]
    }
   ],
   "source": [
    "# best_ckpt = \"/kaggle/working/best.pt\"\n",
    "# model = VisionTextModel(vit_encoder, proj, text_decoder).to(device)\n",
    "# model = nn.DataParallel(model)\n",
    "# state = torch.load(best_ckpt, map_location=device)\n",
    "# model.load_state_dict(state[\"model_state_dict\"])\n",
    "# print(f\"Loaded best checkpoint from {best_ckpt}\")\n",
    "# if tokenizer.bos_token_id is not None:\n",
    "#     model.module.gpt2.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batch = next(iter(test_loader))\n",
    "    img, input_ids, attention_mask, labels = batch\n",
    "    \n",
    "    image_index = 7  \n",
    "    single_img = img[image_index:image_index+1].to(device, non_blocking=True)\n",
    "    single_labels = labels[image_index:image_index+1]\n",
    "    \n",
    "    encoder_outputs = model.module.vit(pixel_values=single_img)\n",
    "    projected = model.module.proj(encoder_outputs.last_hidden_state)\n",
    "    \n",
    "    encoder_attention_mask = torch.ones(\n",
    "        projected.size()[:-1], \n",
    "        dtype=torch.long, \n",
    "        device=projected.device\n",
    "    )\n",
    "    \n",
    "    start_token = tokenizer.bos_token_id if tokenizer.bos_token_id else tokenizer.eos_token_id\n",
    "    input_ids_start = torch.full((1, 1), start_token, dtype=torch.long, device=device)\n",
    "    \n",
    "    generated_ids = model.module.gpt2.generate(\n",
    "        input_ids=input_ids_start,\n",
    "        encoder_hidden_states=projected,        \n",
    "        encoder_attention_mask=encoder_attention_mask, \n",
    "        \n",
    "        # Generation settings for quality output\n",
    "        max_new_tokens=64,\n",
    "        do_sample=True,                         \n",
    "        repetition_penalty=1.2,                # Prevent loops\n",
    "        no_repeat_ngram_size=3,               # Prevent n-gram repetition\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "# Decode results\n",
    "labels_for_decode = single_labels.detach().clone().to('cpu')\n",
    "IGNORE_INDEX = -100\n",
    "pad_id = tokenizer.pad_token_id\n",
    "if pad_id is None:\n",
    "    pad_id = tokenizer.eos_token_id\n",
    "\n",
    "labels_for_decode[labels_for_decode == IGNORE_INDEX] = pad_id\n",
    "labels_for_decode = labels_for_decode.to(torch.long).tolist()\n",
    "label_texts = tokenizer.batch_decode(labels_for_decode, skip_special_tokens=True)\n",
    "\n",
    "pred_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "# Print result for single sample\n",
    "print(f\"\\n--- SINGLE IMAGE RESULTS ---\")\n",
    "print(f\"Predicted: {pred_texts[0]}\")\n",
    "print(\"________________________\")\n",
    "print(f\"Ground Truth: {label_texts[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Federated Learning Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T14:56:39.367643Z",
     "iopub.status.busy": "2025-08-09T14:56:39.366907Z",
     "iopub.status.idle": "2025-08-09T14:56:39.371756Z",
     "shell.execute_reply": "2025-08-09T14:56:39.371232Z",
     "shell.execute_reply.started": "2025-08-09T14:56:39.367619Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Split the data into 2 clients\n",
    "c_train_ds,c2_train_ds = random_split(train_ds, [0.5, 0.5], generator=torch.Generator().manual_seed(42))\n",
    "c1_train_loader,c2_train_loader = DataLoader(c1_train_ds,batch_size=32, shuffle=True), DataLoader(c2_train_ds,batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T14:56:40.647382Z",
     "iopub.status.busy": "2025-08-09T14:56:40.646803Z",
     "iopub.status.idle": "2025-08-09T14:56:40.657789Z",
     "shell.execute_reply": "2025-08-09T14:56:40.657134Z",
     "shell.execute_reply.started": "2025-08-09T14:56:40.647357Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_local_model(model, train_loader,local_optimizer, epochs=2):\n",
    " \n",
    "    for epoch in range(epochs):\n",
    "    # ---- TRAIN ----\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "    \n",
    "        for step, batch in tqdm.tqdm(\n",
    "            enumerate(train_loader, start=1),\n",
    "            total=len(train_loader),\n",
    "            leave=False,\n",
    "            desc=f\"Train {epoch+1}\"\n",
    "        ):\n",
    "            img, input_ids, attention_mask, labels = batch\n",
    "            img = img.to(device, non_blocking=True)\n",
    "            input_ids = input_ids.to(device, non_blocking=True)\n",
    "            attention_mask = attention_mask.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "    \n",
    "            with torch.amp.autocast(device_type=\"cuda\",enabled=use_fp16):\n",
    "                outputs = model(\n",
    "                    pixel_values=img,\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                if loss.dim() > 0:\n",
    "                    loss = loss.mean()\n",
    "                loss = loss / grad_accum_steps\n",
    "    \n",
    "            scaler.scale(loss).backward()\n",
    "            running_loss += loss.item() * grad_accum_steps\n",
    "    \n",
    "            if step % grad_accum_steps == 0:\n",
    "                scaler.unscale_(local_optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "                scaler.step(local_optimizer)\n",
    "                scaler.update()\n",
    "                local_optimizer.zero_grad()\n",
    "    \n",
    "        epoch_train_loss = running_loss / max(1, len(train_loader))\n",
    "    return model.state_dict(),epoch_train_loss    \n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model,test_loader):\n",
    "\n",
    "    model.eval()\n",
    "    test_running_loss = 0.0\n",
    "    test_batches = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm.tqdm(\n",
    "            test_loader,\n",
    "            total=len(test_loader),\n",
    "            leave=False\n",
    "        ):\n",
    "            img, input_ids, attention_mask, labels = batch\n",
    "            img = img.to(device, non_blocking=True)\n",
    "            input_ids = input_ids.to(device, non_blocking=True)\n",
    "            attention_mask = attention_mask.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            with torch.amp.autocast(device_type=\"cuda\",enabled=use_fp16):\n",
    "                outputs = model(\n",
    "                    pixel_values=img,\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                loss = outputs.loss\n",
    "                if loss.dim() > 0:\n",
    "                    loss = loss.mean()\n",
    "\n",
    "            test_running_loss += loss.item()\n",
    "            test_batches += 1\n",
    "\n",
    "    epoch_test_loss = test_running_loss / max(1, test_batches)\n",
    "    print(f\"avg_test_loss = {epoch_test_loss:.4f}\")\n",
    "    \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T14:56:40.833006Z",
     "iopub.status.busy": "2025-08-09T14:56:40.832779Z",
     "iopub.status.idle": "2025-08-09T14:56:40.837260Z",
     "shell.execute_reply": "2025-08-09T14:56:40.836550Z",
     "shell.execute_reply.started": "2025-08-09T14:56:40.832989Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def FedAvg(global_model, client_models):\n",
    "    global_dict = global_model.state_dict()\n",
    "    for k in global_dict.keys():\n",
    "        global_dict[k] = torch.stack([client_models[i][k].float() for i in range(len(client_models))], dim=0).mean(0)\n",
    "    global_model.load_state_dict(global_dict)\n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-09T14:56:43.124142Z",
     "iopub.status.busy": "2025-08-09T14:56:43.123867Z",
     "iopub.status.idle": "2025-08-09T16:21:34.386319Z",
     "shell.execute_reply": "2025-08-09T16:21:34.385467Z",
     "shell.execute_reply.started": "2025-08-09T14:56:43.124120Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/855227569.py:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training parameters:101805696\n",
      "Round: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train 1:   0%|          | 0/118 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 2.8155\n",
      "Round: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 2.3669\n",
      "Round: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 2.1740\n",
      "Round: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 2.0640\n",
      "Round: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.9855\n",
      "Round: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.9257\n",
      "Round: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.8807\n",
      "Round: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.8424\n",
      "Round: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.8161\n",
      "Round: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.7918\n",
      "Round: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.7707\n",
      "Round: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.7520\n",
      "Round: 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.7408\n",
      "Round: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.7293\n",
      "Round: 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.7128\n",
      "Round: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.7037\n",
      "Round: 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.6948\n",
      "Round: 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.6887\n",
      "Round: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.6887\n",
      "Round: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.6765\n",
      "Round: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.6819\n",
      "Round: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.6754\n",
      "Round: 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.6694\n",
      "Round: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.6725\n",
      "Round: 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.6808\n",
      "Round: 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.6740\n",
      "Round: 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.6756\n",
      "Round: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.6743\n",
      "Round: 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.6753\n",
      "Round: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_test_loss = 1.6852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import tqdm as tqdm\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "global_model = VisionTextModel(vit_encoder, proj, text_decoder).to(device)\n",
    "\n",
    "global_model = nn.DataParallel(global_model)\n",
    "\n",
    "print(f\"Total training parameters:{sum(p.numel() for p in global_model.parameters())}\")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "for p in global_model.module.vit.parameters():\n",
    "    p.requires_grad=False \n",
    "\n",
    "clients = [c1_train_loader, c2_train_loader]\n",
    "num_clients = len(clients)\n",
    "rounds = 30\n",
    "for round_no in range(rounds):\n",
    "    print(f\"Round: {round_no+1}\")\n",
    "    client_models = []\n",
    "\n",
    "    for train_loader in clients:\n",
    "        local_model = nn.DataParallel(VisionTextModel(vit_encoder, proj, text_decoder).to(device))\n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "        for p in local_model.module.vit.parameters():\n",
    "            p.requires_grad=False\n",
    "        \n",
    "        local_optimizer = torch.optim.AdamW(local_model.parameters(), lr=1e-5)\n",
    "        client_state_dict,epoch_loss = train_local_model(local_model,train_loader,local_optimizer,epochs=1)\n",
    "        client_models.append(client_state_dict)\n",
    "        del local_model \n",
    "        torch.cuda.empty_cache() \n",
    "\n",
    "    global_model = FedAvg(global_model, client_models)\n",
    "\n",
    "    evaluate(global_model,test_loader)        "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8032115,
     "sourceId": 12708743,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 423700,
     "modelInstanceId": 405792,
     "sourceId": 512619,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 424097,
     "modelInstanceId": 406174,
     "sourceId": 513520,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
