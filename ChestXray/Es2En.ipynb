{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-07T13:05:37.890424Z",
     "iopub.status.busy": "2025-08-07T13:05:37.890045Z",
     "iopub.status.idle": "2025-08-07T13:05:39.626475Z",
     "shell.execute_reply": "2025-08-07T13:05:39.625506Z",
     "shell.execute_reply.started": "2025-08-07T13:05:37.890390Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/padchestpa-1980/PadChestPA_1980.csv\n",
      "/kaggle/input/padchestpa/PadChestPA.csv\n",
      "/kaggle/input/translations/all_translations.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "l =[]\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        l.append(os.path.join(dirname, filename))\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T13:05:48.767772Z",
     "iopub.status.busy": "2025-08-07T13:05:48.767473Z",
     "iopub.status.idle": "2025-08-07T13:06:25.925333Z",
     "shell.execute_reply": "2025-08-07T13:06:25.924251Z",
     "shell.execute_reply.started": "2025-08-07T13:05:48.767749Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 13:06:03.812621: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754571964.114358      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754571964.200164      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "from transformers.pipelines.pt_utils import KeyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T13:06:35.361394Z",
     "iopub.status.busy": "2025-08-07T13:06:35.360868Z",
     "iopub.status.idle": "2025-08-07T13:06:35.427450Z",
     "shell.execute_reply": "2025-08-07T13:06:35.426439Z",
     "shell.execute_reply.started": "2025-08-07T13:06:35.361361Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9496"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(l[0])\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T13:06:38.315558Z",
     "iopub.status.busy": "2025-08-07T13:06:38.315205Z",
     "iopub.status.idle": "2025-08-07T13:06:38.322577Z",
     "shell.execute_reply": "2025-08-07T13:06:38.321668Z",
     "shell.execute_reply.started": "2025-08-07T13:06:38.315535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "def clean_segment(text):\n",
    "    \"\"\"\n",
    "    Clean raw text by removing extra dots, spaces, and formatting issues\n",
    "    \"\"\"\n",
    "    # Split into lines for processing\n",
    "    lines = text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Skip empty lines\n",
    "        if not line.strip():\n",
    "            continue\n",
    "\n",
    "        # Remove multiple consecutive spaces and replace with single space\n",
    "        line = re.sub(r' +', ' ', line)\n",
    "\n",
    "        # Remove multiple consecutive dots and replace with single dot\n",
    "        line = re.sub(r'\\.{2,}', '.', line)\n",
    "\n",
    "        # Remove isolated dots (dots surrounded by spaces)\n",
    "        line = re.sub(r'\\s+\\.\\s+', ' ', line)\n",
    "\n",
    "        # Remove dots at the beginning of lines\n",
    "        line = re.sub(r'^\\s*\\.\\s*', '', line)\n",
    "\n",
    "        # Clean up any remaining multiple spaces\n",
    "        line = re.sub(r' +', ' ', line)\n",
    "\n",
    "        # Strip leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "\n",
    "        # Add cleaned line if it's not empty\n",
    "        if line:\n",
    "            cleaned_lines.append(line)\n",
    "\n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T13:06:49.835999Z",
     "iopub.status.busy": "2025-08-07T13:06:49.835691Z",
     "iopub.status.idle": "2025-08-07T13:06:49.989959Z",
     "shell.execute_reply": "2025-08-07T13:06:49.989127Z",
     "shell.execute_reply.started": "2025-08-07T13:06:49.835979Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/4211703445.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_clean['Report'] = df_clean['Report'].apply(clean_segment)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9490"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df.dropna()\n",
    "df_clean['Report'] = df_clean['Report'].apply(clean_segment)\n",
    "reports = df_clean['Report'].to_list()\n",
    "# dataset = Dataset.from_pandas(pd.DataFrame({\"text\": reports}))\n",
    "len(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T11:23:42.617575Z",
     "iopub.status.busy": "2025-08-06T11:23:42.617324Z",
     "iopub.status.idle": "2025-08-06T11:24:49.445943Z",
     "shell.execute_reply": "2025-08-06T11:24:49.445047Z",
     "shell.execute_reply.started": "2025-08-06T11:23:42.617557Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d37ee5676b44698ac7ca36bb9ace020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/808 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67c403a29b9b41eb8baf1da27068170a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/5.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff37ca775c0d46649874564933cbca9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/5.48G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69dd3cfb84e44145afc5290579372a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/189 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "456bb3c57e504ca6a591f9ddb385f4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/564 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403e276fafb84172aa860f3188eb61a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1b882c25a846c5a3e9b94db37afe0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c89db8399542838fe0612ddb81f9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "model_name = \"facebook/nllb-200-1.3B\"\n",
    "\n",
    "translator = pipeline(\n",
    "    \"translation\", \n",
    "    model=model_name, \n",
    "    device=0,\n",
    "    src_lang=\"spa_Latn\",  # Spanish\n",
    "    tgt_lang=\"eng_Latn\"   # English\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T11:25:19.312871Z",
     "iopub.status.busy": "2025-08-06T11:25:19.312583Z",
     "iopub.status.idle": "2025-08-06T12:57:14.005034Z",
     "shell.execute_reply": "2025-08-06T12:57:14.004215Z",
     "shell.execute_reply.started": "2025-08-06T11:25:19.312851Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting translation and saving...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating and saving batches:   3%|▎         | 9/297 [03:21<1:47:58, 22.50s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Translating and saving batches: 100%|██████████| 297/297 [1:31:54<00:00, 18.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 9490 translations. File saved at: /kaggle/working/all_translations.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "output_folder = \"/kaggle/working\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "batch_size = 32\n",
    "translation_count = 0\n",
    "\n",
    "output_path = os.path.join(output_folder, \"all_translations.txt\") #pass it back \n",
    "print(\"Starting translation and saving...\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "    for idx in tqdm(range(0, len(reports), batch_size), desc=\"Translating and saving batches\"):\n",
    "        batch = reports[idx:idx + batch_size]\n",
    "        translations_batch = translator(batch, max_length=256)\n",
    "        \n",
    "        for translated in translations_batch:\n",
    "            output_file.write(translated['translation_text'] + \"\\n\")\n",
    "        output_file.flush()  # flush once per batch for speed\n",
    "        translation_count += len(translations_batch)\n",
    "\n",
    "print(f\"Completed {translation_count} translations. File saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T07:31:21.950535Z",
     "iopub.status.busy": "2025-08-07T07:31:21.950005Z",
     "iopub.status.idle": "2025-08-07T07:31:21.956637Z",
     "shell.execute_reply": "2025-08-07T07:31:21.956095Z",
     "shell.execute_reply.started": "2025-08-07T07:31:21.950514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_medical_prompt(medical_text):\n",
    "    return f\"\"\"\n",
    "You are a board-certified radiologist with expertise in reviewing and standardizing medical documentation. Your task is to revise the following chest X-ray report to improve its clarity, professional tone, and grammatical accuracy.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Correct all grammatical errors and typos.\n",
    "2.  Ensure the use of standard, concise medical terminology.\n",
    "3.  Improve the overall structure and flow for better readability.\n",
    "4.  Crucially, DO NOT add, invent, or speculate on any medical findings that are not explicitly mentioned in the original text. Your role is to refine the existing information, not to make a new diagnosis.\n",
    "5.  Output ONLY the revised medical report text and nothing else.\n",
    "\n",
    "**Original Report:**\n",
    "\"{medical_text}\"\n",
    "\n",
    "**Revised Report:**\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T07:34:59.696122Z",
     "iopub.status.busy": "2025-08-07T07:34:59.695834Z",
     "iopub.status.idle": "2025-08-07T07:34:59.745575Z",
     "shell.execute_reply": "2025-08-07T07:34:59.744957Z",
     "shell.execute_reply.started": "2025-08-07T07:34:59.696103Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "\n",
    "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# File paths\n",
    "INPUT_FILE = \"/kaggle/input/translations/all_translations.txt\" # file that was generatef earlier using NLLB\n",
    "OUTPUT_FILE = \"/kaggle/working/revised_reports.csv\"\n",
    "\n",
    "BATCH_SIZE = 1 if torch.cuda.is_available() else 1 \n",
    "MAX_LENGTH = 512  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T07:31:34.534313Z",
     "iopub.status.busy": "2025-08-07T07:31:34.533996Z",
     "iopub.status.idle": "2025-08-07T07:34:43.137665Z",
     "shell.execute_reply": "2025-08-07T07:34:43.136904Z",
     "shell.execute_reply.started": "2025-08-07T07:31:34.534295Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df476811014b46e6a36567bf548ef537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1916004bba1748f699ec9f004436626b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf509a43801413d9dea004672cb7c9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf64d3b376354cc9bab7b61cc703338a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2370925d396440e8aaeb0ece400f44a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-07 07:31:45.004506: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754551905.349524      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754551905.446998      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "853c5828313e4047be474edc7fc0c164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88266cbfbf3c4268ba780fac422e506f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa726c12a133477891a9ce0d31bee423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3890a4979c234022a2a6653902264059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae40d48029c844609595fe956e925c15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d0148a77df49fab35457acc49eb2d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcab1641516441398c892fded211edf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded with float16 precision!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Loading model:\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"auto\" if torch.cuda.is_available() else \"cpu\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "        \n",
    "print(\"Model loaded with float16 precision!\")\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T07:34:43.139459Z",
     "iopub.status.busy": "2025-08-07T07:34:43.138553Z",
     "iopub.status.idle": "2025-08-07T07:34:43.145964Z",
     "shell.execute_reply": "2025-08-07T07:34:43.145315Z",
     "shell.execute_reply.started": "2025-08-07T07:34:43.139439Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def process_batch(batch_texts):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for text in batch_texts:\n",
    "        try:\n",
    "            prompt = create_medical_prompt(text)\n",
    "            \n",
    "            inputs = tokenizer(\n",
    "                prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH,\n",
    "                padding=True\n",
    "            )\n",
    "            \n",
    "            # Move to device\n",
    "            if torch.cuda.is_available():\n",
    "                inputs = inputs.to(\"cuda\")\n",
    "            \n",
    "            with torch.inference_mode():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=150,\n",
    "                    temperature=0.3,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            \n",
    "            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            improved = generated[len(prompt):].strip()\n",
    "            \n",
    "            if improved.startswith(\"Improved report:\") or improved.startswith(\"Report:\"):\n",
    "                improved = improved.split(\":\", 1)[1].strip()\n",
    "            \n",
    "            results.append(improved if improved else text)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text: {str(e)[:100]}...\")\n",
    "            results.append(text)  # Fallback to original\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T07:34:43.147514Z",
     "iopub.status.busy": "2025-08-07T07:34:43.147320Z",
     "iopub.status.idle": "2025-08-07T07:34:43.184792Z",
     "shell.execute_reply": "2025-08-07T07:34:43.184291Z",
     "shell.execute_reply.started": "2025-08-07T07:34:43.147499Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9490 medical texts\n"
     ]
    }
   ],
   "source": [
    "with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "    lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "        \n",
    "print(f\"Found {len(lines)} medical texts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-07T07:35:07.808682Z",
     "iopub.status.busy": "2025-08-07T07:35:07.808440Z",
     "iopub.status.idle": "2025-08-07T12:21:14.772666Z",
     "shell.execute_reply": "2025-08-07T12:21:14.771883Z",
     "shell.execute_reply.started": "2025-08-07T07:35:07.808663Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9490/9490 [4:46:06<00:00,  1.81s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Complete!\n",
      "Successfully processed 9490 medical texts\n",
      "Results saved to: /kaggle/working/revised_reports.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import tqdm\n",
    "import torch\n",
    "\n",
    "processed_count = 0\n",
    "\n",
    "with open(OUTPUT_FILE, 'w', newline='', encoding='utf-8') as out_f:\n",
    "    writer = csv.writer(out_f)\n",
    "    \n",
    "    writer.writerow([\"Row\", \"Original Text\", \"Corrected Text\"]) # return a csv file with these 3 columns for comparision\n",
    "    \n",
    "    for i in tqdm.tqdm(range(0, len(lines), BATCH_SIZE)):\n",
    "        batch = lines[i:i + BATCH_SIZE]\n",
    "        \n",
    "        improved_texts = process_batch(batch)\n",
    "\n",
    "        # Save results\n",
    "        for original, improved in zip(batch, improved_texts):\n",
    "            processed_count += 1\n",
    "            writer.writerow([\n",
    "                processed_count,\n",
    "                original.strip(),\n",
    "                improved.strip()\n",
    "            ])\n",
    "\n",
    "        # Clear GPU cache to save menory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nProcessing Complete!\")\n",
    "print(f\"Successfully processed {processed_count} medical texts\")\n",
    "print(f\"Results saved to: {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8019169,
     "sourceId": 12689576,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8019676,
     "sourceId": 12690271,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8022051,
     "sourceId": 12693637,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31091,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
